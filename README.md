# Immersion in the GitHub Universe: Scaling Coding Agents to Mastery

<div align="center">

[![arXiv](https://img.shields.io/badge/arXiv-2602.09892-b31b1b.svg)](https://arxiv.org/abs/2602.09892)
[![Hugging Face Datasets](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Datasets-blue)](https://huggingface.co/datasets/Awe-AI/Scale-SWE)
[![Hugging Face Models](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Models-yellow)](https://huggingface.co/Awe-AI/Scale-SWE)
[![License](https://img.shields.io/badge/License-CC%20BY%204.0-green.svg)](LICENSE)
<br>

**ðŸš§ Release Notice:** The datasets and models are currently undergoing internal approval. <br>
We expect to open-source them by **March 4, 2026**.
</div>



![](figures/scale_swe.drawio.png)

## ðŸ”¥ Highlights

- Source from 6M+ pull requests and 23000+ repositories.
- Cover 5200 Repositories.
- 100k high-quality instances.
- 71k trajectories from DeepSeek v3.2 with 3.5B token.
- Strong performance: 64% in SWE-bench-Verified trained from Qwen3-30A3B-Instruct.

## ðŸ¤– Results
We fine-tuned Qwen-30B-A3B-Instruct on our synthesized trajectories.
![](figures/performance.png)

## ðŸ“– Citation

If you find this project useful for your research, please consider citing our paper:
```
@misc{zhao2026immersiongithubuniversescaling,
      title={Immersion in the GitHub Universe: Scaling Coding Agents to Mastery}, 
      author={Jiale Zhao and Guoxin Chen and Fanzhe Meng and Minghao Li and Jie Chen and Hui Xu and Yongshuai Sun and Xin Zhao and Ruihua Song and Yuan Zhang and Peng Wang and Cheng Chen and Jirong Wen and Kai Jia},
      year={2026},
      eprint={2602.09892},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2602.09892}, 
}
```

## ðŸ“„ License

This project is licensed under the CC BY 4.0 License - see the [LICENSE](LICENSE) file for details.